{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码共有5个板块：导入模块、全局参数、回归模型、创建类、main\n",
    "\n",
    "导入模块：包含所有包的调用。\n",
    "\n",
    "全局参数：包含所有全局参数（路径、画图参数）等的调整。\n",
    "\n",
    "回归模型：机器学习建模的函数，主要在class中被调用。\n",
    "\n",
    "创建类：将代码和功能打包，主要包括：\n",
    "- 类的赋值self.\\__init\\__()：如何接受到因子和收益率数据、按怎样的频率滚动和扩展窗口、训练集测试集的起止时间设置；\n",
    "- 赋值后的样本外收益率预测self.predict_ret()：根据训练集，通过不同的机器学习方法训练模型，并计算出样本外预测值；\n",
    "- 计算样本外的R2 self.cal_oos()：根据样本外预测值和真实值的差异来计算R2，公式为：\n",
    "$$R^2_{oos} = 1 - \\frac{\\sum(r_{i,t+1} - \\hat r_{i,t+1})^2}{\\sum r_{i,t+1}^2} $$\n",
    "\n",
    "main：代码主函数，将class实例化，并计算不同模型的样本外R2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入模块 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:11:12.099301Z",
     "iopub.status.busy": "2020-12-06T03:11:12.098302Z",
     "iopub.status.idle": "2020-12-06T03:11:13.284572Z",
     "shell.execute_reply": "2020-12-06T03:11:13.283582Z",
     "shell.execute_reply.started": "2020-12-06T03:11:12.099301Z"
    }
   },
   "outputs": [],
   "source": [
    "import os # 管理路径\n",
    "import math\n",
    "from tqdm import tqdm,trange # 显示进度条\n",
    "from datetime import timedelta\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd \n",
    "from pandas.tseries.offsets import Day, MonthEnd, MonthBegin # 时间处理\n",
    "import numpy as np\n",
    "\n",
    "# 机器学习模型\n",
    "from sklearn import linear_model\n",
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#import baostock as bs\n",
    "#from WindPy import *\n",
    "#lg = bs.login()\n",
    "#w.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:11:13.294547Z",
     "iopub.status.busy": "2020-12-06T03:11:13.294547Z",
     "iopub.status.idle": "2020-12-06T03:11:13.303526Z",
     "shell.execute_reply": "2020-12-06T03:11:13.301528Z",
     "shell.execute_reply.started": "2020-12-06T03:11:13.294547Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置路径\n",
    "#############################################请修改#########################################################\n",
    "datapath = r'E:\\终生学习\\职业素养\\学术论文\\【202010】emperical asset pricing via machine learning\\data' # 数据存储路径\n",
    "###########################################################################################################\n",
    "pd.options.display.max_columns = 10 # notebook中最大显示列数\n",
    "pd.options.display.max_rows = 10 # notebook中最大显示行数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:11:13.877770Z",
     "iopub.status.busy": "2020-12-06T03:11:13.877770Z",
     "iopub.status.idle": "2020-12-06T03:11:13.949554Z",
     "shell.execute_reply": "2020-12-06T03:11:13.948555Z",
     "shell.execute_reply.started": "2020-12-06T03:11:13.877770Z"
    }
   },
   "outputs": [],
   "source": [
    "# 超参调整，训练模型，这里的函数是不同的模型进行调参。\n",
    "\n",
    "# elastic net\n",
    "def ENReg(X_train, y_train, X_valid, y_valid):\n",
    "    best_en = ElasticNet(alpha=1.0, l1_ratio=1).fit(X_train, y_train) # 先用1.0和1为参数训练模型\n",
    "    best_en_score = best_en.score(X_valid, y_valid) # 得到该参数下的得分\n",
    "    for i in range(1, 20): # 调整超参\n",
    "        alpha_counter = math.pow(0.1, i)\n",
    "        for r in range(21): # 调整超参\n",
    "            l1_ratio_counter = r * 0.05\n",
    "            en_temp = ElasticNet(alpha=alpha_counter, l1_ratio=l1_ratio_counter).fit(X_train, y_train) # 用此时的超参训练模型\n",
    "            if en_temp.score(X_valid, y_valid) > best_en_score: # 如果新模型的得分高于之前的得分，则取代之前的模型\n",
    "                best_alpha = alpha_counter # 最高得分下的超参alpha\n",
    "                best_l1_ratio = l1_ratio_counter # 最高得分下的超参l1_ratio\n",
    "                best_en = ElasticNet(alpha=alpha_counter, l1_ratio=l1_ratio_counter).fit(X_train, y_train) # 用最好的模型训练模型\n",
    "                best_en_score = best_en.score(X_valid, y_valid) # 得到最高的得分\n",
    "    return best_en # 返回训练的最好的模型\n",
    "\n",
    "#PLS\n",
    "def PLSReg(X_train, y_train, X_valid, y_valid):\n",
    "    pls_best = PLSRegression(n_components=1).fit(X_train, y_train) # 先用n_components = 1来训练模型\n",
    "    for i in range(1, min(X_train.shape[1],100)): # 调整参数\n",
    "        pls_temp = PLSRegression(n_components=i).fit(X_train, y_train) \n",
    "        if pls_temp.score(X_valid, y_valid) > pls_best.score(X_valid, y_valid): # 训练出最好的参数\n",
    "            best_pls_conponents = i\n",
    "            pls_best = PLSRegression(n_components=best_pls_conponents).fit(X_train, y_train)\n",
    "    return pls_best\n",
    "\n",
    "#GL待优化\n",
    "def group_lasso(X, y, alpha, groups, max_iter=1000, rtol=1e-6,\n",
    "             verbose=False):\n",
    "    \"\"\"\n",
    "    Linear least-squares with l2/l1 regularization solver.\n",
    "    Solves problem of the form:\n",
    "               .5 * |Xb - y| + n_samples * alpha * Sum(w_j * |b_j|)\n",
    "    where |.| is the l2-norm and b_j is the coefficients of b in the\n",
    "    j-th group. This is commonly known as the `group lasso`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        Design Matrix.\n",
    "    y : array of shape (n_samples,)\n",
    "    alpha : float or array\n",
    "        Amount of penalization to use.\n",
    "    groups : array of shape (n_features,)\n",
    "        Group label. For each column, it indicates\n",
    "        its group apertenance.\n",
    "    rtol : float\n",
    "        Relative tolerance. ensures ||(x - x_) / x_|| < rtol,\n",
    "        where x_ is the approximate solution and x is the\n",
    "        true solution.\n",
    "    Returns\n",
    "    -------\n",
    "    x : array\n",
    "        vector of coefficients\n",
    "    References\n",
    "    ----------\n",
    "    \"Efficient Block-coordinate Descent Algorithms for the Group Lasso\",\n",
    "    Qin, Scheninberg, Goldfarb\n",
    "    \"\"\"\n",
    "\n",
    "    # .. local variables ..\n",
    "    # 检查赋值的格式是否正确\n",
    "    X, y, groups, alpha = map(np.asanyarray, (X, y, groups, alpha))\n",
    "    if len(groups) != X.shape[1]:\n",
    "        raise ValueError(\"Incorrect shape for groups\")\n",
    "    w_new = np.zeros(X.shape[1], dtype=X.dtype)\n",
    "    alpha = alpha * X.shape[0]\n",
    "\n",
    "    # .. use integer indices for groups ..\n",
    "    # 调整超参\n",
    "    group_labels = [np.where(groups == i)[0] for i in np.unique(groups)]\n",
    "    H_groups = [np.dot(X[:, g].T, X[:, g]) for g in group_labels]\n",
    "    eig = list(map(linalg.eigh, H_groups))\n",
    "    Xy = np.dot(X.T, y)\n",
    "    initial_guess = np.zeros(len(group_labels))\n",
    "\n",
    "    def f(x, qp2, eigvals, alpha):\n",
    "        return 1 - np.sum( qp2 / ((x * eigvals + alpha) ** 2))\n",
    "    def df(x, qp2, eigvals, penalty):\n",
    "        # .. first derivative ..\n",
    "        return np.sum((2 * qp2 * eigvals) / ((penalty + x * eigvals) ** 3))\n",
    "\n",
    "    if X.shape[0] > X.shape[1]:\n",
    "        H = np.dot(X.T, X)\n",
    "    else:\n",
    "        H = None\n",
    "\n",
    "    for n_iter in range(max_iter):\n",
    "        w_old = w_new.copy()\n",
    "        for i, g in enumerate(group_labels):\n",
    "            # .. shrinkage operator ..\n",
    "            eigvals, eigvects = eig[i]\n",
    "            w_i = w_new.copy()\n",
    "            w_i[g] = 0.\n",
    "            if H is not None:\n",
    "                X_residual = np.dot(H[g], w_i) - Xy[g]\n",
    "            else:\n",
    "                X_residual = np.dot(X.T, np.dot(X[:, g], w_i)) - Xy[g]\n",
    "            qp = np.dot(eigvects.T, X_residual)\n",
    "            if len(g) < 2:\n",
    "                # for single groups we know a closed form solution\n",
    "                w_new[g] = - np.sign(X_residual) * max(abs(X_residual) - alpha, 0)\n",
    "            else:\n",
    "                if alpha < linalg.norm(X_residual, 2):\n",
    "                    initial_guess[i] = optimize.newton(f, initial_guess[i], df, tol=.5,\n",
    "                                args=(qp ** 2, eigvals, alpha))\n",
    "                    w_new[g] = - initial_guess[i] * np.dot(eigvects /  (eigvals * initial_guess[i] + alpha), qp)\n",
    "                else:\n",
    "                    w_new[g] = 0.\n",
    "\n",
    "\n",
    "        # .. dual gap ..\n",
    "        max_inc = linalg.norm(w_old - w_new, np.inf)\n",
    "        if True: #max_inc < rtol * np.amax(w_new):\n",
    "            residual = np.dot(X, w_new) - y\n",
    "            group_norm = alpha * np.sum([linalg.norm(w_new[g], 2)\n",
    "                         for g in group_labels])\n",
    "            if H is not None:\n",
    "                norm_Anu = [linalg.norm(np.dot(H[g], w_new) - Xy[g]) \\\n",
    "                           for g in group_labels]\n",
    "            else:\n",
    "                norm_Anu = [linalg.norm(np.dot(H[g], residual)) \\\n",
    "                           for g in group_labels]\n",
    "            if np.any(norm_Anu > alpha):\n",
    "                nnu = residual * np.min(alpha / norm_Anu)\n",
    "            else:\n",
    "                nnu = residual\n",
    "            primal_obj =  .5 * np.dot(residual, residual) + group_norm\n",
    "            dual_obj   = -.5 * np.dot(nnu, nnu) - np.dot(nnu, y)\n",
    "            dual_gap = primal_obj - dual_obj\n",
    "            if verbose:\n",
    "                print('Relative error: %s' % (dual_gap / dual_obj))\n",
    "            if np.abs(dual_gap / dual_obj) < rtol:\n",
    "                break\n",
    "\n",
    "    return w_new\n",
    "\n",
    "# Random forest\n",
    "def RFReg(X_train, y_train, X_valid, y_valid):\n",
    "    rf_best = RandomForestRegressor().fit(X_train, y_train)\n",
    "    for L in range(1,10): # 调整超参L\n",
    "        for B in range(1,10): # 调整超参B\n",
    "            rf_temp = RandomForestRegressor(n_estimators=10, max_depth=None).fit(X_train, y_train)\n",
    "            if rf_temp.score(X_valid, y_valid)> rf_best.score(X_valid, y_valid): # 保留使得模型得分最好的B\\L\n",
    "                L_best = L\n",
    "                B_best = B\n",
    "                rf_best = RandomForestRegressor(n_estimators=B_best, max_depth=L_best).fit(X_train, y_train) # 用最好的参数训练模型\n",
    "    return rf_best\n",
    "\n",
    "# gradient boosting regression trees\n",
    "def GBRTReg(X_train, y_train, X_valid, y_valid):\n",
    "    gbrt_best = GradientBoostingRegressor(loss='huber').fit(X_train, y_train) \n",
    "    for L in range(1, 5): # 调整超参L\n",
    "        for v in range(1,5): # 调整超参learning_rate\n",
    "            learning_rate = v *0.1\n",
    "            for B in range(20,100,10): # 调整超参B\n",
    "                gbrt_temp = GradientBoostingRegressor(loss='huber',learning_rate=learning_rate, n_estimators=B, max_depth=L).fit(X_train, y_train)\n",
    "                if gbrt_temp.score(X_valid, y_valid)> gbrt_best.score(X_valid, y_valid):\n",
    "                    L_best = L\n",
    "                    learning_rate_best = learning_rate\n",
    "                    B_best = B\n",
    "                    gbrt_best = GradientBoostingRegressor(loss='huber',learning_rate=learning_rate_best, n_estimators=B_best, max_depth=L_best).fit(X_train, y_train)\n",
    "    return gbrt_best # 返回得分最高的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:11:14.877141Z",
     "iopub.status.busy": "2020-12-06T03:11:14.876140Z",
     "iopub.status.idle": "2020-12-06T03:11:14.916033Z",
     "shell.execute_reply": "2020-12-06T03:11:14.915073Z",
     "shell.execute_reply.started": "2020-12-06T03:11:14.876140Z"
    }
   },
   "outputs": [],
   "source": [
    "class Factor_models(object):\n",
    "    '''\n",
    "    params:\n",
    "    -------\n",
    "        chardf:个股特征值的面板数据，index为日期格式，columns为features。\n",
    "        retdf:个股收益率的面板数据，index为日期格式。\n",
    "        first_train_end_date:初次训练时，训练集的截止日期。（开始日期默认为chardf和retdf的第一期）。格式为'%Y%m%d'。\n",
    "        last_train_end_date:最后一次训练时，训练集的截止日期。格式为'%Y%m%d'。\n",
    "        freq:按什么频率expanding训练集，默认为'm'，每个月滚动一次。\n",
    "    \n",
    "    methods:\n",
    "    --------\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,chardf,retdf,first_train_end_date,last_train_end_date,freq='m'):\n",
    "        '''\n",
    "        参数初始化\n",
    "        '''\n",
    "        self._chardf = chardf # 个股特征值的面板数据\n",
    "        self._retdf = retdf # 个股收益率的面板数据\n",
    "        self.first_train_end_date = first_train_end_date # 初次训练时，训练集的截止日期。之后训练集每月expanding。\n",
    "        self.last_train_end_date = last_train_end_date # 最后一次训练时，训练集的截止日期。即样本的截止日期。\n",
    "        self.freq = freq # 训练集expanding的频率，是按月还是按年还是其他？\n",
    "        \n",
    "    def predict_ret(self):\n",
    "        # 创建一个train_end_list，训练集每月expanding。\n",
    "        train_end_list = pd.date_range(self.first_train_end_date,self.last_train_end_date,freq=self.freq) # freq=m，按月度滚动\n",
    "        preddf = pd.DataFrame() # 存储不同模型预测出来的y值，即存储样本外预测收益率的值\n",
    "        for end_date in tqdm(train_end_list): # 通过逐月改变训练集end_date的方法，切割样本\n",
    "            # end_date = train_end_list[0]  \n",
    "            valid_date = end_date + MonthEnd() # 验证集为训练集后的第一个月，每次训练完之后预测一个月\n",
    "            end_date = end_date.strftime('%Y%m%d') # 转化成字符型\n",
    "            valid_date = valid_date.strftime('%Y%m%d') # 转化为字符型\n",
    "\n",
    "            # 划分集合\n",
    "            # 训练集\n",
    "            train_x = chardf.loc[:end_date] # 训练集为期初至end_date，end_date是按月滚动的值\n",
    "            train_y = retdf.loc[:end_date] # 训练集为期初至end_date，end_date是按月滚动的值\n",
    "\n",
    "            # 测试集\n",
    "            valid_x = chardf.loc[valid_date:valid_date] # 测试集为训练集的次月，只预测下个月的月度收益率\n",
    "            valid_y = retdf.loc[valid_date:valid_date] # 测试集为训练集的次月，只预测下个月的月度收益率\n",
    "\n",
    "            # 数据标准化? 这里有问题。。。\n",
    "            # train_x = (train_x - train_x.mean()) / np.std(train_x)\n",
    "            # train_y = (train_y - train_y.mean()) / np.std(train_y)\n",
    "\n",
    "            # 建模预测收益率\n",
    "            ## 先创建一个临时的temp_preddf,用来存储当前月份的验证集下的real y和不同模型的预测y\n",
    "            temp_preddf = pd.DataFrame() # 创建当前训练集下训练出的predict y和real y\n",
    "            temp_preddf['real_y'] = valid_y.iloc[:,0]  # real_y就是验证集valid_y的第一列。因为valid_y是真实收益率数据在vailid_date上的切割\n",
    "\n",
    "            ## OLS，已优化\n",
    "            reg = linear_model.LinearRegression() # 根据OLS模型预测收益率\n",
    "            reg.fit(train_x,train_y)\n",
    "            predict_y = reg.predict(valid_x)\n",
    "            temp_preddf['OLS_y'] = predict_y # 新建一列OLS_y存储OLS预测出的收益率\n",
    "\n",
    "            ## PLS，已优化\n",
    "            pls = PLSReg(train_x, train_y, valid_x, valid_y) # 根据PLS模型预测收益率\n",
    "            predict_y = pls.predict(valid_x)\n",
    "            temp_preddf['PLS_y'] = predict_y # 新建一列PLS_y存储PLS预测出的收益率\n",
    "\n",
    "            ## PCR，待优化\n",
    "            pca = PCA(n_components='mle',svd_solver='full') # 根据PCR模型预测收益率\n",
    "            pca.fit(train_x)\n",
    "            temp = pca.transform(train_x)\n",
    "            lr = linear_model.LinearRegression()\n",
    "            lr.fit(temp,train_y)   # 模型训练\n",
    "            predict_y = lr.predict(pca.transform(valid_x))\n",
    "            temp_preddf['PCR_y'] = predict_y # 新建一列PCR_y存储PCR预测出的收益率\n",
    "\n",
    "            ## Elastic Net，已优化\n",
    "            enet = ENReg(train_x, train_y, valid_x, valid_y) # 根据elastic net模型预测收益率\n",
    "            predict_y = enet.predict(valid_x)\n",
    "            temp_preddf['ENet_y'] = predict_y # 新建一列enet_y存储PCR预测出的收益率\n",
    "\n",
    "            ## lasso，已优化\n",
    "            lasso = linear_model.Lasso() # 根据lasso模型预测收益率\n",
    "            param = [{'alpha':np.linspace(0,1,100)}]\n",
    "            grid = GridSearchCV(lasso, param_grid=param) # 网格法调整超参\n",
    "            grid.fit(train_x,train_y)\n",
    "            predict_y = grid.predict(valid_x)\n",
    "            temp_preddf['lasso_y'] = predict_y # 新建一列lasso_y存储lasso预测出的收益率\n",
    "\n",
    "            ## GLM with group lasso，还没搞懂\n",
    "\n",
    "            ## RF, 已优化\n",
    "            clf = RFReg(train_x, train_y, valid_x, valid_y)  # 根据random forest模型预测收益率\n",
    "            predict_y = clf.predict(valid_x)\n",
    "            temp_preddf['rf_y'] = predict_y\n",
    "\n",
    "            ## GBRT，已优化\n",
    "            gbrt = GBRTReg(train_x, train_y, valid_x, valid_y)  # 根据GBRT模型预测收益率\n",
    "            predict_y = gbrt.predict(valid_x)\n",
    "            temp_preddf['gbrt_y'] = predict_y\n",
    "\n",
    "            ## NN1,未调参\n",
    "            ### 一层的神经网络\n",
    "            regr = MLPRegressor(hidden_layer_sizes=3, random_state=1, max_iter=500)  # 超参未调\n",
    "            regr.fit(train_x, train_y)\n",
    "            predict_y = regr.predict(valid_x)\n",
    "            temp_preddf['NN1_y'] = predict_y\n",
    "\n",
    "            ## NN2,未调参\n",
    "            ### 两层的神经网络\n",
    "            regr = MLPRegressor(hidden_layer_sizes=4, random_state=1, max_iter=500) # 超参未调\n",
    "            regr.fit(train_x, train_y)\n",
    "            predict_y = regr.predict(valid_x)\n",
    "            temp_preddf['NN2_y'] = predict_y\n",
    "\n",
    "            ## NN3,未调参\n",
    "            ### 三层的神经网络\n",
    "            regr = MLPRegressor(hidden_layer_sizes=5, random_state=1, max_iter=500) # 超参未调\n",
    "            regr.fit(train_x, train_y)\n",
    "            predict_y = regr.predict(valid_x)\n",
    "            temp_preddf['NN3_y'] = predict_y\n",
    "\n",
    "            ## NN4,未调参\n",
    "            ### 四层的神经网络\n",
    "            regr = MLPRegressor(hidden_layer_sizes=6, random_state=1, max_iter=500) # 超参未调\n",
    "            regr.fit(train_x, train_y)\n",
    "            predict_y = regr.predict(valid_x)\n",
    "            temp_preddf['NN4_y'] = predict_y\n",
    "\n",
    "            ## NN5,未调参\n",
    "            ### 五层的神经网络\n",
    "            regr = MLPRegressor(hidden_layer_sizes=7, random_state=1, max_iter=500) # 超参未调\n",
    "            regr.fit(train_x, train_y)\n",
    "            predict_y = regr.predict(valid_x)\n",
    "            temp_preddf['NN5_y'] = predict_y\n",
    "\n",
    "            ## 将temp_preddf并入preddf\n",
    "            preddf = preddf.append(temp_preddf) # 将当前valid_date下得到的predict_y和real_y一起并入preddf中\n",
    "            self._preddf = preddf\n",
    "        return preddf # 最后我们只返回preddf，也就是所有期的predict y和real y\n",
    "    \n",
    "    def cal_oos(self):\n",
    "        # 计算out-of-sample R2 根据代码开头的公式\n",
    "        try:\n",
    "            preddf = self._preddf # 如果self已经有self._preddf，即self.predict_ret()已经运行过了，已经预测过收益率了，则无需再次运行。\n",
    "        except:\n",
    "            preddf = self.predict_ret() # 如果之前没有运行过self.predict_ret()，则需要运行。\n",
    "        denominator = (preddf['real_y'] ** 2).sum() # 分母是真实收益率的平方和\n",
    "        numerator = preddf.apply(lambda x: preddf['real_y'] - x).iloc[:,1:] # 分子是real_y - predict_y的平方和\n",
    "        numerator = (numerator ** 2).sum()\n",
    "        \n",
    "        roos = 1 - numerator / denominator # 再用 1 减去分子/分母\n",
    "        roos.index = roos.index.str.rstrip('_y') # 之前的index都是模型_y，比如\"OLS_y\"，不美观，删除_y。\n",
    "        fig,ax = plt.subplots(figsize = (16,12)) # 画图，将不同模型的Roos画出来。\n",
    "        plt.title('Out-of-sample predicting R2', fontsize = 20)\n",
    "        ax.bar(x = roos.index, height = roos)\n",
    "        plt.show()\n",
    "        return roos # 返回样本外Roos，这个Roos是不同模型对应的样本外R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数，主函数里首先设置好工作路径，并读取因子数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:12:20.887451Z",
     "iopub.status.busy": "2020-12-06T03:12:20.887451Z",
     "iopub.status.idle": "2020-12-06T03:12:21.083484Z",
     "shell.execute_reply": "2020-12-06T03:12:21.082487Z",
     "shell.execute_reply.started": "2020-12-06T03:12:20.887451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data: 8 items\n",
      "                  code      mret  size  bm\n",
      "date                                     \n",
      "2010-01-31  000022.SZ       NaN   NaN NaN\n",
      "2010-02-28  000022.SZ       NaN   NaN NaN\n",
      "2010-03-31  000022.SZ       NaN   NaN NaN\n",
      "2010-04-30  000022.SZ       NaN   NaN NaN\n",
      "2010-05-31  000022.SZ       NaN   NaN NaN\n",
      "2010-06-30  000022.SZ       NaN   NaN NaN\n",
      "2012-02-29  600102.SH -0.140056   NaN NaN\n",
      "2018-12-31  600270.SH -5.747643   NaN NaN\n"
     ]
    }
   ],
   "source": [
    "# 导入chardf, retdf数据，并做好缺失值处理\n",
    "os.chdir(datapath) # 将路径设置为《全局参数》中的datapath\n",
    "totaldf = pd.read_csv('中证500_20100101_20200930月度因子数据.csv',index_col=0) # 读取因子数据\n",
    "totaldf = totaldf.set_index('date') # 将date设置为index\n",
    "totaldf.index = pd.to_datetime(totaldf.index)\n",
    "# 缺失值处理\n",
    "## 首先看一下缺失值是哪些\n",
    "print('Missing data: {} items\\n'.format(len(totaldf[totaldf.isna().any(1)])), totaldf[totaldf.isna().any(1)]) # 看一下缺失值是哪些行\n",
    "## 缺失值不多，简单处理可以直接剔除\n",
    "totaldf = totaldf.dropna(how='any') # 直接删除\n",
    "chardf = totaldf.iloc[:,2:] # features\n",
    "retdf = totaldf[['mret']] # monthly ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:12:22.014242Z",
     "iopub.status.busy": "2020-12-06T03:12:22.013246Z",
     "iopub.status.idle": "2020-12-06T03:12:22.040144Z",
     "shell.execute_reply": "2020-12-06T03:12:22.039146Z",
     "shell.execute_reply.started": "2020-12-06T03:12:22.014242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>bm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>1.225713e+10</td>\n",
       "      <td>0.420704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>5.878727e+09</td>\n",
       "      <td>0.442315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>8.116641e+09</td>\n",
       "      <td>0.160954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>2.504960e+09</td>\n",
       "      <td>0.365256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>3.374784e+09</td>\n",
       "      <td>0.401930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>3.646009e+10</td>\n",
       "      <td>0.056334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>4.573680e+10</td>\n",
       "      <td>0.056051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>3.778278e+10</td>\n",
       "      <td>0.038479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>1.855394e+10</td>\n",
       "      <td>0.095617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>1.739654e+10</td>\n",
       "      <td>0.102545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64492 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    size        bm\n",
       "date                              \n",
       "2010-01-31  1.225713e+10  0.420704\n",
       "2010-01-31  5.878727e+09  0.442315\n",
       "2010-01-31  8.116641e+09  0.160954\n",
       "2010-01-31  2.504960e+09  0.365256\n",
       "2010-01-31  3.374784e+09  0.401930\n",
       "...                  ...       ...\n",
       "2020-09-30  3.646009e+10  0.056334\n",
       "2020-09-30  4.573680e+10  0.056051\n",
       "2020-09-30  3.778278e+10  0.038479\n",
       "2020-09-30  1.855394e+10  0.095617\n",
       "2020-09-30  1.739654e+10  0.102545\n",
       "\n",
       "[64492 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chardf # 因子df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:12:31.625788Z",
     "iopub.status.busy": "2020-12-06T03:12:31.625788Z",
     "iopub.status.idle": "2020-12-06T03:12:31.636770Z",
     "shell.execute_reply": "2020-12-06T03:12:31.636770Z",
     "shell.execute_reply.started": "2020-12-06T03:12:31.625788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>24.410774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>14.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>13.866540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>-7.829978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>-11.498810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>-6.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>-13.054112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>-4.731282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>-16.881531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>-17.679901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64492 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mret\n",
       "date                 \n",
       "2010-01-31  24.410774\n",
       "2010-01-31  14.727273\n",
       "2010-01-31  13.866540\n",
       "2010-01-31  -7.829978\n",
       "2010-01-31 -11.498810\n",
       "...               ...\n",
       "2020-09-30  -6.347826\n",
       "2020-09-30 -13.054112\n",
       "2020-09-30  -4.731282\n",
       "2020-09-30 -16.881531\n",
       "2020-09-30 -17.679901\n",
       "\n",
       "[64492 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retdf # 月度收益率df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:12:54.915748Z",
     "iopub.status.busy": "2020-12-06T03:12:54.914754Z",
     "iopub.status.idle": "2020-12-06T03:12:54.919736Z",
     "shell.execute_reply": "2020-12-06T03:12:54.919246Z",
     "shell.execute_reply.started": "2020-12-06T03:12:54.915748Z"
    }
   },
   "outputs": [],
   "source": [
    "# 赋值 其中train_set是expanding window,初始为20100131至20131231,之后每月扩展,valid_set是1个月的rolling window即train_set的下一个月\n",
    "# 这里的last_train_end_date最高可以放到倒数第二期，在我的数据里即为20200831,用前10年数据训练模型来预测最后一期的r\n",
    "# 但如果将last_train_end_date设置为20200831,需要运行4个小时左右。\n",
    "basic_2_factors = Factor_models(chardf, retdf, first_train_end_date='20131231', last_train_end_date='20140331', freq='m') # 将class实例化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:12:55.456510Z",
     "iopub.status.busy": "2020-12-06T03:12:55.456510Z",
     "iopub.status.idle": "2020-12-06T03:23:18.986613Z",
     "shell.execute_reply": "2020-12-06T03:23:18.986613Z",
     "shell.execute_reply.started": "2020-12-06T03:12:55.456510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [10:23<00:00, 155.79s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算样本外R2，运行耗时较长\n",
    "roos = basic_2_factors.cal_oos() # 计算不同模型样本外R2。self.cal_oos()中已经包含了self.predict_ret()的操作，先通过不同的模型预测收益率，再比较样本外真实收益率和预测收益率的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:24:08.755700Z",
     "iopub.status.busy": "2020-12-06T03:24:08.754710Z",
     "iopub.status.idle": "2020-12-06T03:24:08.782606Z",
     "shell.execute_reply": "2020-12-06T03:24:08.781609Z",
     "shell.execute_reply.started": "2020-12-06T03:24:08.755700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_y</th>\n",
       "      <th>OLS_y</th>\n",
       "      <th>PLS_y</th>\n",
       "      <th>PCR_y</th>\n",
       "      <th>ENet_y</th>\n",
       "      <th>...</th>\n",
       "      <th>NN1_y</th>\n",
       "      <th>NN2_y</th>\n",
       "      <th>NN3_y</th>\n",
       "      <th>NN4_y</th>\n",
       "      <th>NN5_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>-3.309353</td>\n",
       "      <td>-1.102139</td>\n",
       "      <td>-1.915149</td>\n",
       "      <td>1.072854</td>\n",
       "      <td>-1.102139</td>\n",
       "      <td>...</td>\n",
       "      <td>1.524180</td>\n",
       "      <td>0.905840</td>\n",
       "      <td>1.215264</td>\n",
       "      <td>1.184254</td>\n",
       "      <td>0.849959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>-1.718213</td>\n",
       "      <td>-2.698197</td>\n",
       "      <td>-3.445749</td>\n",
       "      <td>-0.438182</td>\n",
       "      <td>-2.698197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997728</td>\n",
       "      <td>0.220658</td>\n",
       "      <td>1.104066</td>\n",
       "      <td>1.184254</td>\n",
       "      <td>0.617411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>-1.968504</td>\n",
       "      <td>-2.010228</td>\n",
       "      <td>-3.116305</td>\n",
       "      <td>1.020463</td>\n",
       "      <td>-2.010228</td>\n",
       "      <td>...</td>\n",
       "      <td>1.505927</td>\n",
       "      <td>0.722103</td>\n",
       "      <td>1.211408</td>\n",
       "      <td>1.184254</td>\n",
       "      <td>0.842165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>-3.663793</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>-0.053970</td>\n",
       "      <td>2.454937</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>...</td>\n",
       "      <td>2.005704</td>\n",
       "      <td>1.596813</td>\n",
       "      <td>1.316972</td>\n",
       "      <td>1.184254</td>\n",
       "      <td>1.062553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>-14.989733</td>\n",
       "      <td>0.977981</td>\n",
       "      <td>-1.074857</td>\n",
       "      <td>5.863970</td>\n",
       "      <td>0.977981</td>\n",
       "      <td>...</td>\n",
       "      <td>3.193427</td>\n",
       "      <td>2.519208</td>\n",
       "      <td>1.567846</td>\n",
       "      <td>1.184254</td>\n",
       "      <td>1.588249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>-1.033231</td>\n",
       "      <td>1.605075</td>\n",
       "      <td>1.608639</td>\n",
       "      <td>1.373109</td>\n",
       "      <td>1.570506</td>\n",
       "      <td>...</td>\n",
       "      <td>1.649842</td>\n",
       "      <td>1.180452</td>\n",
       "      <td>1.562396</td>\n",
       "      <td>1.184879</td>\n",
       "      <td>-0.959424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>-13.211845</td>\n",
       "      <td>1.141416</td>\n",
       "      <td>1.262925</td>\n",
       "      <td>0.695535</td>\n",
       "      <td>1.074969</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408109</td>\n",
       "      <td>0.952408</td>\n",
       "      <td>1.475372</td>\n",
       "      <td>1.184879</td>\n",
       "      <td>-0.857063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>11.386139</td>\n",
       "      <td>4.450256</td>\n",
       "      <td>4.389508</td>\n",
       "      <td>3.940560</td>\n",
       "      <td>4.374298</td>\n",
       "      <td>...</td>\n",
       "      <td>2.565811</td>\n",
       "      <td>2.256790</td>\n",
       "      <td>1.892144</td>\n",
       "      <td>1.184879</td>\n",
       "      <td>-1.347646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>-8.638743</td>\n",
       "      <td>3.127010</td>\n",
       "      <td>3.181155</td>\n",
       "      <td>2.541525</td>\n",
       "      <td>3.039757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.066688</td>\n",
       "      <td>1.714575</td>\n",
       "      <td>1.712460</td>\n",
       "      <td>1.184879</td>\n",
       "      <td>-1.136173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>8.781609</td>\n",
       "      <td>5.924623</td>\n",
       "      <td>5.525391</td>\n",
       "      <td>6.006973</td>\n",
       "      <td>5.936895</td>\n",
       "      <td>...</td>\n",
       "      <td>3.303029</td>\n",
       "      <td>2.964028</td>\n",
       "      <td>2.157542</td>\n",
       "      <td>1.184879</td>\n",
       "      <td>-1.659838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               real_y     OLS_y     PLS_y     PCR_y    ENet_y  ...     NN1_y  \\\n",
       "date                                                           ...             \n",
       "2014-01-31  -3.309353 -1.102139 -1.915149  1.072854 -1.102139  ...  1.524180   \n",
       "2014-01-31  -1.718213 -2.698197 -3.445749 -0.438182 -2.698197  ...  0.997728   \n",
       "2014-01-31  -1.968504 -2.010228 -3.116305  1.020463 -2.010228  ...  1.505927   \n",
       "2014-01-31  -3.663793  0.700272 -0.053970  2.454937  0.700272  ...  2.005704   \n",
       "2014-01-31 -14.989733  0.977981 -1.074857  5.863970  0.977981  ...  3.193427   \n",
       "...               ...       ...       ...       ...       ...  ...       ...   \n",
       "2014-04-30  -1.033231  1.605075  1.608639  1.373109  1.570506  ...  1.649842   \n",
       "2014-04-30 -13.211845  1.141416  1.262925  0.695535  1.074969  ...  1.408109   \n",
       "2014-04-30  11.386139  4.450256  4.389508  3.940560  4.374298  ...  2.565811   \n",
       "2014-04-30  -8.638743  3.127010  3.181155  2.541525  3.039757  ...  2.066688   \n",
       "2014-04-30   8.781609  5.924623  5.525391  6.006973  5.936895  ...  3.303029   \n",
       "\n",
       "               NN2_y     NN3_y     NN4_y     NN5_y  \n",
       "date                                                \n",
       "2014-01-31  0.905840  1.215264  1.184254  0.849959  \n",
       "2014-01-31  0.220658  1.104066  1.184254  0.617411  \n",
       "2014-01-31  0.722103  1.211408  1.184254  0.842165  \n",
       "2014-01-31  1.596813  1.316972  1.184254  1.062553  \n",
       "2014-01-31  2.519208  1.567846  1.184254  1.588249  \n",
       "...              ...       ...       ...       ...  \n",
       "2014-04-30  1.180452  1.562396  1.184879 -0.959424  \n",
       "2014-04-30  0.952408  1.475372  1.184879 -0.857063  \n",
       "2014-04-30  2.256790  1.892144  1.184879 -1.347646  \n",
       "2014-04-30  1.714575  1.712460  1.184879 -1.136173  \n",
       "2014-04-30  2.964028  2.157542  1.184879 -1.659838  \n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看不同模型在不同月份的样本外预测ret\n",
    "basic_2_factors._preddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T03:24:10.867884Z",
     "iopub.status.busy": "2020-12-06T03:24:10.866864Z",
     "iopub.status.idle": "2020-12-06T03:24:10.877834Z",
     "shell.execute_reply": "2020-12-06T03:24:10.876837Z",
     "shell.execute_reply.started": "2020-12-06T03:24:10.867884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLS      0.020053\n",
       "PLS      0.018920\n",
       "PCR      0.017590\n",
       "ENet     0.029590\n",
       "lasso    0.015401\n",
       "           ...   \n",
       "NN1      0.004113\n",
       "NN2      0.019791\n",
       "NN3     -0.018000\n",
       "NN4     -0.017171\n",
       "NN5      0.014176\n",
       "Length: 12, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 样本外预测r2\n",
    "roos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
